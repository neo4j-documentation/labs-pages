= LlamaIndex
:slug: llamaindex
:author: Michael Hunger, Tomaz Bratanic
:category: labs
:tags: llama index, llm, framework, python, vector, cypher generation
:neo4j-versions: 5.x
:page-pagination:
:page-product: llamaindex

// image::todo.png[width=800]

LlamaIndex is a popular LLM orchestration framework with a clean architecture and a focus on data structures and models.
It integrates many LLMs as well as vector stores and other indexes and contains tooling for document loading (loader hub) and advanced RAG patterns.

LlamaIndex provides a lot of detailed examples for GenAI application development in their https://blog.llamaindex.ai/[blogs^] and https://docs.llamaindex.ai[documentation^].

The Neo4j integration covers both the vector store as well as query generation from natural language and knowledge graph construction.

== Functionality Includes

=== Neo4jVector

The Neo4j Vector integration supports a number of operations

* create vector from langchain documents
* query vector
* query vector with additional graph retrieval Cypher query
* construct vector instance from existing graph data
* hybrid search
* metadata filtering

[source,python]
----
%pip install llama-index-llms-openai
%pip install llama-index-vector-stores-neo4jvector
%pip install llama-index-embeddings-openai
%pip install neo4j

import os
import openai
from llama_index.vector_stores.neo4jvector import Neo4jVectorStore
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

os.environ["OPENAI_API_KEY"] = "OPENAI_API_KEY"
openai.api_key = os.environ["OPENAI_API_KEY"]
username = "neo4j"
password = "pleaseletmein"
url = "bolt://localhost:7687"
embed_dim = 1536

neo4j_vector = Neo4jVectorStore(username, password, url, embed_dim)
# load documents
documents = SimpleDirectoryReader("./data/paul_graham").load_data()
from llama_index.core import StorageContext

storage_context = StorageContext.from_defaults(vector_store=neo4j_vector)
index = VectorStoreIndex.from_documents(
    documents, storage_context=storage_context
)

query_engine = index.as_query_engine()
response = query_engine.query("What happened at interleaf?")
----

==== Hybrid search

Hybrid search combines vector search with fulltext search with re-ranking and de-duplication of the results.

[source,python]
----
neo4j_vector_hybrid = Neo4jVectorStore(
    username, password, url, embed_dim, hybrid_search=True
)

storage_context = StorageContext.from_defaults(
    vector_store=neo4j_vector_hybrid
)
index = VectorStoreIndex.from_documents(
    documents, storage_context=storage_context
)
query_engine = index.as_query_engine()
response = query_engine.query("What happened at interleaf?")
----

==== Metadata filtering

Metadata filtering enhances vector search by allowing searches to be refined based on specific node properties.
This integrated approach ensures more precise and relevant search results by leveraging both the vector similarities and the contextual attributes of the nodes.

[source,python]
----
from llama_index.core.vector_stores import (
    MetadataFilter,
    MetadataFilters,
    FilterOperator,
)

filters = MetadataFilters(
    filters=[
        MetadataFilter(
            key="theme", operator=FilterOperator.EQ, value="Fiction"
        ),
    ]
)

retriever = index.as_retriever(filters=filters)
retriever.retrieve("What is inception about?")
----

=== Neo4jGraphStore

The Neo4j Graph Store integration is a wrapper for the Neo4j Python driver. 
It allows querying and updating the Neo4j database in a simplified manner from LlamaIndex.
Many integrations allow you to use the Neo4j Graph Store as a source of data for LlamaIndex.

==== Knowledge graph index

Knowledge graph index can be used to extract graph representation of information from text and use it to construct a knowledge graph.
The graph information can then be retrieved in a RAG application for more accurate responses.

[source,python]
----
%pip install llama-index-llms-openai
%pip install llama-index-graph-stores-neo4j
%pip install llama-index-embeddings-openai
%pip install neo4j

from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    KnowledgeGraphIndex,
    Settings
)
from llama_index.graph_stores.neo4j import Neo4jGraphStore

llm = OpenAI(temperature=0, model="gpt-3.5-turbo")
embedding_llm = OpenAIEmbedding(model="text-embedding-ada-002")

Settings.llm = llm
Settings.embed_model = embedding_llm
Settings.chunk_size = 512

documents = SimpleDirectoryReader(
    "../../../../examples/paul_graham_essay/data"
).load_data()

graph_store = Neo4jGraphStore(username=username,password=password,
    url=url,database=database)

storage_context = StorageContext.from_defaults(graph_store=graph_store)

index = KnowledgeGraphIndex.from_documents(documents,
    storage_context=storage_context, max_triplets_per_chunk=2,
    include_embeddings=True
)

query_engine = index.as_query_engine(
    include_text=True,
    response_mode="tree_summarize",
    embedding_mode="hybrid",
    similarity_top_k=5,
)

response = query_engine.query(
    "Tell me more about what the author worked on at Interleaf"
)
----

==== Knowledge graph query engine

The Knowledge Graph Query Engine generated Cypher statements based on natural language input to retrieve information from the knowledge graph.

[source,python]
----
%pip install llama-index-llms-openai
%pip install llama-index-graph-stores-neo4j
%pip install llama-index-embeddings-openai
%pip install neo4j

from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.query_engine import KnowledgeGraphQueryEngine
from llama_index.graph_stores.neo4j import Neo4jGraphStore

llm=OpenAI(model_name="gpt-3.5-turbo")
service_context = ServiceContext.from_defaults(llm=llm, chunk_size=256)
graph_store = Neo4jGraphStore(username=username,password=password,
    url=url,database=database)

storage_context = StorageContext.from_defaults(graph_store=graph_store)
query_engine = KnowledgeGraphQueryEngine(
    storage_context=storage_context,
    service_context=service_context,
    llm=llm,
    verbose=True,
    refresh_schema=True
)

response = query_engine.query(
    "Tell me more about what the author worked on at Interleaf",
)
----

== Documentation

* https://docs.llamaindex.ai/en/latest/examples/index_structs/knowledge_graph/Neo4jKGIndexDemo.html[Neo4jKGIndexDemo^]
* https://docs.llamaindex.ai/en/stable/examples/vector_stores/Neo4jVectorDemo.html[Neo4jVectorDemo^]

* https://llamahub.ai/l/graphdb_cypher[Cypher Loader^]

=== Neo4j Query Engine Pack

This https://llamahub.ai/l/llama_packs-neo4j_query_engine[Neo4j Query Engine LlamaPack^] creates a Neo4j query engine, and executes its query function. This pack offers the option of creating multiple types of query engines, namely:

* Knowledge graph vector-based entity retrieval (default if no query engine type option is provided)
* Knowledge graph keyword-based entity retrieval
* Knowledge graph hybrid entity retrieval
* Raw vector index retrieval
* Custom combo query engine (vector similarity + KG entity retrieval)
* KnowledgeGraphQueryEngine
* KnowledgeGraphRAGRetriever


== Relevant Links
[cols="1,4"]
|===
| icon:user[] Authors | https://github.com/tomasonjo[Tomaz Bratanic^], https://github.com/jexp[Michael Hunger^]
| icon:comments[] Community Support | https://community.neo4j.com/[Neo4j Online Community^]
| icon:github[] Repository | https://github.com/run-llama/llama-hub/tree/main/llama_hub/tools/neo4j_db[GitHub Neo4jDB^] https://github.com/run-llama/llama-hub/tree/main/llama_hub/llama_packs/neo4j_query_engine[GitHub Neo4j Llama Pack^]
| icon:book[] Documentation | https://docs.llamaindex.ai/en/stable/examples/index_structs/knowledge_graph/Neo4jKGIndexDemo.html
| icon:github[] Starter Kit | https://neo4j.com/labs/genai-ecosystem/llamaindex/[LlamaIndex Starter Kit^]
| icon:book[] Notebook | https://github.com/run-llama/llama-hub/blob/main/llama_hub/llama_packs/neo4j_query_engine/llama_packs_neo4j.ipynb[Llama Pack Notebook^]
|===

== Videos & Tutorials


* https://graphstuff.fm/episodes/llamaindex-and-more-building-llm-tech-with-jerry-liu[GraphStuff.fm Podcast: LlamaIndex and More: Building LLM Tech with Jerry Liu^]

++++
<iframe width="560" height="315" src="https://www.youtube.com/embed/kSAh4H4YDX8?si=hoK1Vrw0Hs4Et5si" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
++++

== Highlighted Articles

* https://blog.llamaindex.ai/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206[Multimodal RAG Pipeline with LlamaIndex and Neo4j^]

* https://blog.llamaindex.ai/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7[Enriching LlamaIndex Models from GraphQL and Graph Databases^]

* https://levelup.gitconnected.com/a-simpler-way-to-query-neo4j-knowledge-graphs-99c0a8bbf1d7[A Simpler Way to Query Neo4j Knowledge Graphs^]

* https://medium.com/@yu-joshua/using-llamaparse-for-knowledge-graph-creation-from-documents-3bd1e1849754[Using LlamaParse for Knowledge Graph Creation from Documents^]
